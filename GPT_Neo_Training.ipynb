{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11590045,"sourceType":"datasetVersion","datasetId":7267556}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install Required Libraries\n!pip install -q transformers datasets accelerate\n\n# Step 2: Imports\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import Dataset\nimport pandas as pd\n\n# Step 3: Device Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Step 4: Load Dataset\ntrain_df = pd.read_csv(\"/kaggle/input/training-data/training_data.csv\")\n\ntrain_df = train_df.rename(columns={\n    \"choice1\": \"choice_0\",\n    \"choice2\": \"choice_1\",\n    \"choice3\": \"choice_2\",\n    \"choice4\": \"choice_3\"\n})\n\n# Step 5: Format Dataset\ndef format_example(example):\n    return f\"Question: {example['question']}\\nOptions:\\nA. {example['choice_0']}\\nB. {example['choice_1']}\\nC. {example['choice_2']}\\nD. {example['choice_3']}\"\n\ntrain_texts = train_df.apply(format_example, axis=1).tolist()\ndataset = Dataset.from_dict({\"text\": train_texts})\n\n# Step 6: Tokenization Function\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\n# Step 7: Define Model Checkpoint & Variants\nmodel_checkpoint = \"EleutherAI/gpt-neo-125M\"\n\nvariants = {\n    \"model-1\":      {\"batch_size\": 1, \"learning_rate\": 1e-5, \"num_epochs\": 1, \"accum_steps\": 8},\n    \"model-2\":   {\"batch_size\": 2, \"learning_rate\": 3e-5, \"num_epochs\": 2, \"accum_steps\": 4},\n    \"model-3\":     {\"batch_size\": 2, \"learning_rate\": 5e-5, \"num_epochs\": 3, \"accum_steps\": 4},\n    \"model-4\": {\"batch_size\": 2, \"learning_rate\": 2e-5, \"num_epochs\": 5, \"accum_steps\": 4},\n    \"model-5\":     {\"batch_size\": 4, \"learning_rate\": 1e-4, \"num_epochs\": 2, \"accum_steps\": 2},\n}\n\n# Step 8: Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Step 9: Tokenize Dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\ntokenized_dataset = tokenized_dataset.with_format(\"torch\")\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Step 10: Training Loop\nfor variant_name, params in variants.items():\n    print(f\"\\nğŸš€ Training {model_checkpoint} with **{variant_name}** variant...\", flush=True)\n\n    output_dir = f\"/kaggle/working/{model_checkpoint.replace('/', '_')}_{variant_name}_model\"\n\n    # Fresh model for each run\n    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n    model.gradient_checkpointing_enable()\n    model = model.to(device)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        save_total_limit=1,\n        save_steps=1000000,\n        save_strategy=\"no\",\n        logging_dir=f\"{output_dir}/logs\",\n        logging_steps=1000000,\n        per_device_train_batch_size=params[\"batch_size\"],\n        gradient_accumulation_steps=params[\"accum_steps\"],\n        learning_rate=params[\"learning_rate\"],\n        num_train_epochs=params[\"num_epochs\"],\n        gradient_checkpointing=True,\n        fp16=True,\n        weight_decay=0.01,\n        report_to=\"none\",\n        dataloader_num_workers=2,\n        warmup_steps=50,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    print(f\"Finished training & saving for {variant_name}!\\n\", flush=True)\n\nprint(\"All GPT-Neo 125M variants trained successfully!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T04:55:57.745560Z","iopub.execute_input":"2025-04-29T04:55:57.746162Z","iopub.status.idle":"2025-04-29T07:06:58.301842Z","shell.execute_reply.started":"2025-04-29T04:55:57.746138Z","shell.execute_reply":"2025-04-29T07:06:58.300923Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-04-29 04:57:21.469776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745902641.672702      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745902641.731446      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"âœ… Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c64ebce2cb7442359abdb96768b25d68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a414f68ce14ac183b9a25e2ecf688b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d803b50dce49bba0fcac5dab4f5c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0caae10024145279fa268584fa61f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05f1f6dcbd74bfa8f2d6e94d04ad03f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13830 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1584ad8701774b04b388c44d22731ff1"}},"metadata":{}},{"name":"stdout","text":"\nğŸš€ Training EleutherAI/gpt-neo-125M with **neo_low** variant...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe9ced392094895a7dbfbb6d9ae2f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e65aa28a61fa4a849a775bf68ca6ceca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5acd1c0112134909a51ac2a83a3ce720"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/3105047695.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1728' max='1728' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1728/1728 16:21, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… Finished training & saving for neo_low!\n\n\nğŸš€ Training EleutherAI/gpt-neo-125M with **neo_medium** variant...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3105047695.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3456' max='3456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3456/3456 19:37, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Finished training & saving for neo_medium!\n\n\nğŸš€ Training EleutherAI/gpt-neo-125M with **neo_high** variant...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3105047695.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5184' max='5184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5184/5184 29:21, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Finished training & saving for neo_high!\n\n\nğŸš€ Training EleutherAI/gpt-neo-125M with **neo_extended** variant...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3105047695.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8640' max='8640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8640/8640 48:50, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Finished training & saving for neo_extended!\n\n\nğŸš€ Training EleutherAI/gpt-neo-125M with **neo_fast** variant...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3105047695.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3458' max='3458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3458/3458 14:46, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Finished training & saving for neo_fast!\n\nğŸ¯ All GPT-Neo 125M variants trained successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:12:45.394383Z","iopub.execute_input":"2025-04-29T07:12:45.394669Z","iopub.status.idle":"2025-04-29T07:14:56.860456Z","shell.execute_reply.started":"2025-04-29T07:12:45.394650Z","shell.execute_reply":"2025-04-29T07:14:56.859457Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/ (stored 0%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/tokenizer.json (deflated 82%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/tokenizer_config.json (deflated 55%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/vocab.json","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/training_args.bin (deflated 52%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/model.safetensors (deflated 8%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/config.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_extended_model/merges.txt (deflated 53%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/ (stored 0%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/tokenizer.json (deflated 82%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/tokenizer_config.json (deflated 55%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/vocab.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/training_args.bin (deflated 51%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/model.safetensors (deflated 8%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/config.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_low_model/merges.txt (deflated 53%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/ (stored 0%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/tokenizer.json (deflated 82%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/tokenizer_config.json (deflated 55%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/vocab.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/training_args.bin (deflated 52%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/model.safetensors (deflated 8%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/config.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_fast_model/merges.txt (deflated 53%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/ (stored 0%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/tokenizer.json (deflated 82%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/tokenizer_config.json (deflated 55%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/vocab.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/training_args.bin (deflated 52%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/model.safetensors (deflated 8%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/config.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_medium_model/merges.txt (deflated 53%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/ (stored 0%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/tokenizer.json (deflated 82%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/tokenizer_config.json (deflated 55%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/vocab.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/training_args.bin (deflated 51%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/model.safetensors (deflated 8%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/config.json (deflated 59%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/EleutherAI_gpt-neo-125M_neo_high_model/merges.txt (deflated 53%)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:17:41.954075Z","iopub.execute_input":"2025-04-29T07:17:41.954396Z","iopub.status.idle":"2025-04-29T07:17:42.136263Z","shell.execute_reply.started":"2025-04-29T07:17:41.954371Z","shell.execute_reply":"2025-04-29T07:17:42.135291Z"}},"outputs":[{"name":"stdout","text":"EleutherAI_gpt-neo-125M_neo_extended_model\nEleutherAI_gpt-neo-125M_neo_fast_model\nEleutherAI_gpt-neo-125M_neo_high_model\nEleutherAI_gpt-neo-125M_neo_low_model\nEleutherAI_gpt-neo-125M_neo_medium_model\nfile.zip\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:18:08.038694Z","iopub.execute_input":"2025-04-29T07:18:08.039013Z","iopub.status.idle":"2025-04-29T07:18:08.044750Z","shell.execute_reply.started":"2025-04-29T07:18:08.038988Z","shell.execute_reply":"2025-04-29T07:18:08.044060Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}],"execution_count":5}]}